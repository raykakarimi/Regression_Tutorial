{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction \n",
    "\n",
    "In this project, the total site electricity usage of a housing unit, in kilowatt-hour, is predicted using some features provided by Residential Energy Consumption Survey (RECS) in 2009.  \n",
    "To develop a comprehensive predictive model, we one-by-one test different regression models arranged from simplest to the most complex and evaluate each.   First of all, the feature correlation is utilized to find the features correlated directly with the  KWH field. If the KWH field is a multiple of one of the features, the correlation will be close to one; consequently, the best predictive model will be found. \n",
    "Secondly, linear regressions ranging from simple regression to polynomial regression will be tested. Finally, if the previous models do not result in an acceptable performance (e.g., the accuracy >0.99), nonlinear models such as Kernel techniques and neural networks are taken into account. The outline of methodolgy for finding the best model is as follows:\n",
    "\n",
    "<ul>\n",
    "<li>Preprocessing and Data Engineering Steps</li>\n",
    "<li>Searching for Axiomatic Solutions</li>\n",
    "<li>Feature Selection for Regression</li>\n",
    "<li>Final Model Architecture</li>\n",
    "</ul>\n",
    "\n",
    "# Methodology\n",
    "## Preprocessing and Data Engineering \n",
    "\n",
    "In this part, preprocessing transformations applied to the complete dataset are described in detail. These steps will be used again within the model pipeline after splitting the dataset into training, validation and test sets.  \n",
    "Before starting data processing, we must first load the dataset in the ensuing block. The directory of the dataset is located beside the codes folder. The dataset, converted to a Pandas DataFrame, is a CSV file so that columns and rows refer to features and samples, respectively. Moreover, required packages for this project are imported below, and the parameters and initial variables are set here. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rayka\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3063: DtypeWarning: Columns (717,718) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline , Pipeline\n",
    "from joblib import dump, load\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression\n",
    "\n",
    "#Dataset address\n",
    "source_path = '../'\n",
    "dataset_dir = 'data'\n",
    "dataset_file_name = 'recs2009_public.csv'\n",
    "dataset_path = os.path.join(source_path + dataset_dir, dataset_file_name)\n",
    "\n",
    "#Best Model Address \n",
    "experiment_number = 1\n",
    "experiment_path = '../data/models/experiment{}/'.format(experiment_number)\n",
    "\n",
    "#Parameters of the model and Variables\n",
    "random_state = 42\n",
    "Number_folds = 5\n",
    "test_size_ratio = 0.33\n",
    "var_threshold_feature_select = 0.6\n",
    "\n",
    "#loading dataset \n",
    "dataset_df_raw = pd.read_csv(dataset_path)\n",
    "dataset_df = dataset_df_raw.copy()\n",
    "\n",
    "#Create a directory for saved model\n",
    "Path(experiment_path).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature's Type ِِDetermination\n",
    "The first step toward preprocessing is to distinguish between categorical and numerical features. To this end, the types of features are printed in this block. The name of features whose type is object are put in a set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOEID                  int64\n",
      "REGIONC                int64\n",
      "DIVISION               int64\n",
      "REPORTABLE_DOMAIN      int64\n",
      "TYPEHUQ                int64\n",
      "                      ...   \n",
      "WSF                  float64\n",
      "OA_LAT                 int64\n",
      "GWT                    int64\n",
      "DesignDBT99            int64\n",
      "DesignDBT1             int64\n",
      "Length: 940, dtype: object\n",
      "The set of categorical features: {'METROMICRO', 'UR', 'NKRGALNC', 'NOCRCASH', 'IECC_Climate_Pub'}\n"
     ]
    }
   ],
   "source": [
    "# Printing types of features\n",
    "print(dataset_df.dtypes)\n",
    "\n",
    "# separating Categorical features\n",
    "cat_feature_df = dataset_df.select_dtypes(include=['object'])\n",
    "categorical_feature = set(cat_feature_df.keys())\n",
    "print(\"The set of categorical features:\",categorical_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Contemplating the value of features spotted by the previous block, we could realize two columns of NKRGALNC and NKRGALNC are numerical, but they have missing values filled by \".\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency of appearance of each Value for NOCRCASH -2    9958\n",
      "-2    2028\n",
      "1       18\n",
      "3       10\n",
      "4       10\n",
      "5        9\n",
      "2        7\n",
      "1        6\n",
      "16       4\n",
      "6        3\n",
      "8        3\n",
      "12       3\n",
      "24       2\n",
      "15       2\n",
      "3        2\n",
      "55       2\n",
      ".        2\n",
      "4        2\n",
      "12       2\n",
      "10       1\n",
      "20       1\n",
      "28       1\n",
      "40       1\n",
      "6        1\n",
      "20       1\n",
      "24       1\n",
      "55       1\n",
      "10       1\n",
      "25       1\n",
      "Name: NOCRCASH, dtype: int64\n",
      "Frequency of appearance of each Value for NKRGALNC -2    9958\n",
      "-2    2028\n",
      "5       42\n",
      "3       11\n",
      "1       10\n",
      "10       8\n",
      "10       5\n",
      "5        5\n",
      "15       2\n",
      ".        2\n",
      "1        2\n",
      "3        1\n",
      "77       1\n",
      "2        1\n",
      "30       1\n",
      "20       1\n",
      "15       1\n",
      "8        1\n",
      "8        1\n",
      "2        1\n",
      "55       1\n",
      "Name: NKRGALNC, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Frequency of appearance of each Value for NOCRCASH\",dataset_df[\"NOCRCASH\"].value_counts())\n",
    "print(\"Frequency of appearance of each Value for NKRGALNC\",dataset_df[\"NKRGALNC\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the below cell, missing values of the two aforementioned columns can be replaced by NaN or -2. These features are converted to the numerical type after fixing the problem caused by missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rayka\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "C:\\Users\\rayka\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "# Filling missing values of two specific columns (NKRGALNC and NKRGALNC)  with -2\n",
    "# pd.to_numeric(dataset_df[\"NKRGALNC\"], errors='coerce') \n",
    "for i, value in enumerate(dataset_df[\"NKRGALNC\"]):\n",
    "    if value == \".\":\n",
    "        dataset_df[\"NKRGALNC\"][i] = -2\n",
    "dataset_df[\"NKRGALNC\"] = pd.to_numeric(dataset_df[\"NKRGALNC\"],downcast='float')\n",
    "\n",
    "for i, value in enumerate(dataset_df[\"NOCRCASH\"]):\n",
    "    if value == \".\":\n",
    "        dataset_df[\"NOCRCASH\"][i] = -2        \n",
    "dataset_df[\"NOCRCASH\"] = pd.to_numeric(dataset_df[\"NOCRCASH\"],downcast='float')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding Categorical Features\n",
    "Most of the regression models support numbers as input variables. In this sense, categorical features should be encoded so that ordinal numbers are assigned to ordered values of a categorical feature. Once the modification occurs,  each feature will be a number. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ord_enc = OrdinalEncoder()\n",
    "for feature in  categorical_feature :\n",
    "    dataset_df[feature] = ord_enc.fit_transform(dataset_df[[feature]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firs of all, the correlation between KWH and other features are derived from the covariance matrix of the features.\n",
    "Then, KWH column is extracted from the main dataset below. Additionally, the dataset is transformed from Pandas Dataframe into a NumPy array since I do not use distributed operating system for processing; moreover, the size of the data is not that big. Training and test sets are separated here as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Covariance Matrix of features  and measure how much KWH is correlated with others\n",
    "feature_cor = dataset_df.corr()[\"KWH\"]\n",
    "KWH_feature_cor = feature_cor.to_numpy()\n",
    "\n",
    "# Extracting the target value from dataset\n",
    "y = np.array(dataset_df.pop(\"KWH\"))\n",
    "\n",
    "# dataframe to numpy array\n",
    "dataset = dataset_df.to_numpy()   \n",
    "\n",
    "#train test split\n",
    "dataset_train, dataset_test, y_train, y_test = train_test_split(dataset, y, test_size = test_size_ratio, random_state = random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "Feature reduction and feature selection are two main tools for dimensionality reduction in data engineering. Feature reduction, e.g., Principal Component Analysis (PCA), is a kind of projection mapping dataset from the main space into another space with a lower dimension. On the other hand, without changing the features, a feature selection module simply selects and removes attributes. Feature selection can be performed in unsupervised or supervised fashions. \n",
    "\n",
    "In this project, both unsupervised feature selection as well as supervised one are only deployed for dimensionality reduction. In this part, features with low variance are removed in an unsupervised manner. In other words, the features' exclusion is happening without paying attention to the target value, KWH field. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed dataset size after unsupervised feature selection (8095, 444)\n"
     ]
    }
   ],
   "source": [
    "sel = VarianceThreshold(threshold=(var_threshold_feature_select * (1 - var_threshold_feature_select)))\n",
    "dataset_train_A = sel.fit_transform(dataset_train)\n",
    "dataset_test_A = sel.transform(dataset_test)\n",
    "\n",
    "#number of features before supervised feature selection \n",
    "print(\"Transformed dataset size after unsupervised feature selection\", dataset_train_A.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature normalization is another preprocessing step used to scale features so that all features stay in the same range ( between zero and one). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling features and normalization \n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "min_max_scaler1 = preprocessing.MinMaxScaler()\n",
    "\n",
    "dataset_train_normalized = min_max_scaler.fit_transform(dataset_train_A)\n",
    "dataset_test_normalized = min_max_scaler.transform(dataset_test_A)\n",
    "\n",
    "y_train_normalized = min_max_scaler1.fit_transform(y_train.reshape(y_train.shape[0],-1))\n",
    "y_test_normalized = min_max_scaler1.transform(y_test.reshape(y_test.shape[0],-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching for Axiomatic Solutions\n",
    "\n",
    "In this section, we are going to find the features which are directly related to the target value. To put it another way, axiomatic solutions of the regression problem are detected here. \n",
    "\n",
    "In previous blocks, correlation between target value and the other features were calculated. The features whose correlation with the target value is approximately 1, are detected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.where(abs(KWH_feature_cor) > 0.9), \"IDs of features whose absolut value of correlation with KWH is higher than 0.9\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After investigation through the codebook of features, the aforementioned arrays, which have the highest correlation with KWH, correspond to the total electricity usage in KWH and BTC, respectively. If BTC columns exist among features, a simple unit converter can be utilized as a predictive model. Hence, we eliminate this feature.  \n",
    "\n",
    "In the next step, we verify whether the summation of some features produces the KWH."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression().fit(dataset_train, y_train)\n",
    "reg.score(dataset_train, y_train)\n",
    "print(np.where(reg.coef_>0.95),\"IDs of features whose linear combination produces KWH\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the correspondence between features' names and indices above, I realized that the element-wise summation of a set of five features including KWHSPH, KWHCOL, KWHWTH, KWHRFG, and KWHOTH is KWH. Furthermore, the summation of BTUELSPH, BTUELCOL, BTUELWTH, and BTUELRFG equals total usage of electricity in BTU. Therefore, An axiomatic relationship between these features and the target value of prediction can be considered as a model. To avoid this fact, we assume that we do not have these features and remove these columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train2 = np.delete(dataset_train, np.s_[839:850],1)\n",
    "dataset_test2 = np.delete(dataset_test, np.s_[839:850],1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection for Regression\n",
    "\n",
    "In this part, SelectKBest, a supervised feature selection function, is applied to the input to select features and regressors. The features are selected based on scores corresponding to their significance for the regression task.  \n",
    "\n",
    "In our model, some univariate linear regression tests are utilized to assign a p-value to each feature. The procedure occurs in 2 steps:\n",
    "<ol>\n",
    "<li>The correlation between each regressor and the target is computed, that is, $\\dfrac{(X[:, i] - mean(X[:, i])) \\times (y - mean_y))}{(std(X[:, i]) \\times std(y)}$.</li>\n",
    "<li>each correlation is converted to an F score and then to a p-value </li>\n",
    "</ol>\n",
    "\n",
    "\n",
    "In the following example, we choose the ten most important features, but the number of selected features varies within the main model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8095, 928)\n",
      "(8095, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rayka\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:299: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  corr /= X_norms\n",
      "C:\\Users\\rayka\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:299: RuntimeWarning: invalid value encountered in true_divide\n",
      "  corr /= X_norms\n",
      "C:\\Users\\rayka\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:304: RuntimeWarning: invalid value encountered in true_divide\n",
      "  F = corr ** 2 / (1 - corr ** 2) * degrees_of_freedom\n",
      "C:\\Users\\rayka\\anaconda3\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py:903: RuntimeWarning: invalid value encountered in greater\n",
      "  return (a < x) & (x < b)\n",
      "C:\\Users\\rayka\\anaconda3\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py:903: RuntimeWarning: invalid value encountered in less\n",
      "  return (a < x) & (x < b)\n",
      "C:\\Users\\rayka\\anaconda3\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py:1912: RuntimeWarning: invalid value encountered in less_equal\n",
      "  cond2 = cond0 & (x <= _a)\n"
     ]
    }
   ],
   "source": [
    "# define feature selection\n",
    "fs = SelectKBest(score_func = f_regression, k = 100)\n",
    "\n",
    "# apply feature selection\n",
    "print(dataset_train2.shape)\n",
    "X_selected = fs.fit_transform(dataset_train2, y_train)\n",
    "print(X_selected.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model Architecture\n",
    "\n",
    "Some steps, such as encoding, are bypassed within the pipeline to simplify the model. \n",
    "We use the result of previous codes for encoding categorical features and removing features causing axiomatic solutions. \n",
    "Min-Max feature normalization is added to the pipeline so we can make sure all features are in the same range (between zero and one). K-fold cross-validation is applied to the training set for tuning hyperparameters of models and fairly comparing the performance of each model. Before applying regression, features are selected in a supervised manner. The number of selected features is another hyperparameter of the model. This parameter's best value will be defined later by inferring the result derived from the cross-validation part.  \n",
    "\n",
    "The least-squares Linear Regression is the first applicable model for this dataset. In this model, the features are linearly projected using  learnable coefficients $ w = (w_1,...,{w}_{number of features})$. These learnable weights are measured by optimizing the residual sum of squares between the projected features and the observed target value. The score used for evaluation is the coefficient of determination $R^2$ of the prediction.\n",
    "The coefficient $R^2$ is defined as $1 - \\dfrac{\\sum{(y_{true} - y_{pred})^2} }{\\sum((y_{true} - y_{true}.mean())^2 }  $. \n",
    "\n",
    "The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a $R^2$ score of 0.0.\n",
    "\n",
    "The pipeline is fit to the training set and  evaulated by validation set across different number of selected features. At each step, the best model with the largest score is cached. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of selected features for regression: 1\n",
      "Number of selected features for regression: 2\n",
      "Number of selected features for regression: 3\n",
      "Number of selected features for regression: 4\n",
      "Number of selected features for regression: 5\n",
      "Number of selected features for regression: 6\n",
      "Number of selected features for regression: 7\n",
      "Number of selected features for regression: 8\n",
      "Number of selected features for regression: 9\n",
      "Number of selected features for regression: 10\n",
      "Number of selected features for regression: 11\n",
      "Number of selected features for regression: 12\n",
      "Number of selected features for regression: 13\n",
      "Number of selected features for regression: 14\n",
      "Number of selected features for regression: 15\n",
      "Number of selected features for regression: 16\n",
      "Number of selected features for regression: 17\n",
      "Number of selected features for regression: 18\n",
      "Number of selected features for regression: 19\n",
      "Number of selected features for regression: 20\n",
      "Number of selected features for regression: 21\n",
      "Number of selected features for regression: 22\n",
      "Number of selected features for regression: 23\n",
      "Number of selected features for regression: 24\n",
      "Number of selected features for regression: 25\n",
      "Number of selected features for regression: 26\n",
      "Number of selected features for regression: 27\n",
      "Number of selected features for regression: 28\n",
      "Number of selected features for regression: 29\n",
      "Number of selected features for regression: 30\n",
      "Number of selected features for regression: 31\n",
      "Number of selected features for regression: 32\n",
      "Number of selected features for regression: 33\n",
      "Number of selected features for regression: 34\n",
      "Number of selected features for regression: 35\n",
      "Number of selected features for regression: 36\n",
      "Number of selected features for regression: 37\n",
      "Number of selected features for regression: 38\n",
      "Number of selected features for regression: 39\n",
      "Number of selected features for regression: 40\n",
      "Number of selected features for regression: 41\n",
      "Number of selected features for regression: 42\n",
      "Number of selected features for regression: 43\n",
      "Number of selected features for regression: 44\n",
      "Number of selected features for regression: 45\n",
      "Number of selected features for regression: 46\n",
      "Number of selected features for regression: 47\n",
      "Number of selected features for regression: 48\n",
      "Number of selected features for regression: 49\n",
      "Number of selected features for regression: 50\n",
      "Number of selected features for regression: 51\n",
      "Number of selected features for regression: 52\n",
      "Number of selected features for regression: 53\n",
      "Number of selected features for regression: 54\n",
      "Number of selected features for regression: 55\n",
      "Number of selected features for regression: 56\n",
      "Number of selected features for regression: 57\n",
      "Number of selected features for regression: 58\n",
      "Number of selected features for regression: 59\n",
      "Number of selected features for regression: 60\n",
      "Number of selected features for regression: 61\n",
      "Number of selected features for regression: 62\n",
      "Number of selected features for regression: 63\n",
      "Number of selected features for regression: 64\n",
      "Number of selected features for regression: 65\n",
      "Number of selected features for regression: 66\n",
      "Number of selected features for regression: 67\n",
      "Number of selected features for regression: 68\n",
      "Number of selected features for regression: 69\n",
      "Number of selected features for regression: 70\n",
      "Number of selected features for regression: 71\n",
      "Number of selected features for regression: 72\n",
      "Number of selected features for regression: 73\n",
      "Number of selected features for regression: 74\n",
      "Number of selected features for regression: 75\n",
      "Number of selected features for regression: 76\n",
      "Number of selected features for regression: 77\n",
      "Number of selected features for regression: 78\n",
      "Number of selected features for regression: 79\n",
      "Number of selected features for regression: 80\n",
      "Number of selected features for regression: 81\n",
      "Number of selected features for regression: 82\n",
      "Number of selected features for regression: 83\n",
      "Number of selected features for regression: 84\n",
      "Number of selected features for regression: 85\n",
      "Number of selected features for regression: 86\n",
      "Number of selected features for regression: 87\n",
      "Number of selected features for regression: 88\n",
      "Number of selected features for regression: 89\n",
      "Number of selected features for regression: 90\n",
      "Number of selected features for regression: 91\n",
      "Number of selected features for regression: 92\n",
      "Number of selected features for regression: 93\n",
      "Number of selected features for regression: 94\n",
      "Number of selected features for regression: 95\n",
      "Number of selected features for regression: 96\n",
      "Number of selected features for regression: 97\n",
      "Number of selected features for regression: 98\n",
      "Number of selected features for regression: 99\n",
      "Number of selected features for regression: 100\n",
      "Number of selected features for regression: 101\n",
      "Number of selected features for regression: 102\n",
      "Number of selected features for regression: 103\n",
      "Number of selected features for regression: 104\n",
      "Number of selected features for regression: 105\n",
      "Number of selected features for regression: 106\n",
      "Number of selected features for regression: 107\n",
      "Number of selected features for regression: 108\n",
      "Number of selected features for regression: 109\n",
      "Number of selected features for regression: 110\n",
      "Number of selected features for regression: 111\n",
      "Number of selected features for regression: 112\n",
      "Number of selected features for regression: 113\n",
      "Number of selected features for regression: 114\n",
      "Number of selected features for regression: 115\n",
      "Number of selected features for regression: 116\n",
      "Number of selected features for regression: 117\n",
      "Number of selected features for regression: 118\n",
      "Number of selected features for regression: 119\n",
      "Number of selected features for regression: 120\n",
      "Number of selected features for regression: 121\n",
      "Number of selected features for regression: 122\n",
      "Number of selected features for regression: 123\n",
      "Number of selected features for regression: 124\n",
      "Number of selected features for regression: 125\n",
      "Number of selected features for regression: 126\n",
      "Number of selected features for regression: 127\n",
      "Number of selected features for regression: 128\n",
      "Number of selected features for regression: 129\n",
      "Number of selected features for regression: 130\n",
      "Number of selected features for regression: 131\n",
      "Number of selected features for regression: 132\n",
      "Number of selected features for regression: 133\n",
      "Number of selected features for regression: 134\n",
      "Number of selected features for regression: 135\n",
      "Number of selected features for regression: 136\n",
      "Number of selected features for regression: 137\n",
      "Number of selected features for regression: 138\n",
      "Number of selected features for regression: 139\n",
      "Number of selected features for regression: 140\n",
      "Number of selected features for regression: 141\n",
      "Number of selected features for regression: 142\n",
      "Number of selected features for regression: 143\n",
      "Number of selected features for regression: 144\n",
      "Number of selected features for regression: 145\n",
      "Number of selected features for regression: 146\n",
      "Number of selected features for regression: 147\n",
      "Number of selected features for regression: 148\n",
      "Number of selected features for regression: 149\n",
      "Number of selected features for regression: 150\n",
      "Number of selected features for regression: 151\n",
      "Number of selected features for regression: 152\n",
      "Number of selected features for regression: 153\n",
      "Number of selected features for regression: 154\n",
      "Number of selected features for regression: 155\n",
      "Number of selected features for regression: 156\n",
      "Number of selected features for regression: 157\n",
      "Number of selected features for regression: 158\n",
      "Number of selected features for regression: 159\n",
      "Number of selected features for regression: 160\n",
      "Number of selected features for regression: 161\n",
      "Number of selected features for regression: 162\n",
      "Number of selected features for regression: 163\n",
      "Number of selected features for regression: 164\n",
      "Number of selected features for regression: 165\n",
      "Number of selected features for regression: 166\n",
      "Number of selected features for regression: 167\n",
      "Number of selected features for regression: 168\n",
      "Number of selected features for regression: 169\n",
      "Number of selected features for regression: 170\n",
      "Number of selected features for regression: 171\n",
      "Number of selected features for regression: 172\n",
      "Number of selected features for regression: 173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of selected features for regression: 174\n",
      "Number of selected features for regression: 175\n",
      "Number of selected features for regression: 176\n",
      "Number of selected features for regression: 177\n",
      "Number of selected features for regression: 178\n",
      "Number of selected features for regression: 179\n",
      "Number of selected features for regression: 180\n",
      "Number of selected features for regression: 181\n",
      "Number of selected features for regression: 182\n",
      "Number of selected features for regression: 183\n",
      "Number of selected features for regression: 184\n",
      "Number of selected features for regression: 185\n",
      "Number of selected features for regression: 186\n",
      "Number of selected features for regression: 187\n",
      "Number of selected features for regression: 188\n",
      "Number of selected features for regression: 189\n",
      "Number of selected features for regression: 190\n",
      "Number of selected features for regression: 191\n",
      "Number of selected features for regression: 192\n",
      "Number of selected features for regression: 193\n",
      "Number of selected features for regression: 194\n",
      "Number of selected features for regression: 195\n",
      "Number of selected features for regression: 196\n",
      "Number of selected features for regression: 197\n",
      "Number of selected features for regression: 198\n",
      "Number of selected features for regression: 199\n",
      "Number of selected features for regression: 200\n",
      "Number of selected features for regression: 201\n",
      "Number of selected features for regression: 202\n",
      "Number of selected features for regression: 203\n",
      "Number of selected features for regression: 204\n",
      "Number of selected features for regression: 205\n",
      "Number of selected features for regression: 206\n",
      "Number of selected features for regression: 207\n",
      "Number of selected features for regression: 208\n",
      "Number of selected features for regression: 209\n",
      "Number of selected features for regression: 210\n",
      "Number of selected features for regression: 211\n",
      "Number of selected features for regression: 212\n",
      "Number of selected features for regression: 213\n",
      "Number of selected features for regression: 214\n",
      "Number of selected features for regression: 215\n",
      "Number of selected features for regression: 216\n",
      "Number of selected features for regression: 217\n",
      "Number of selected features for regression: 218\n",
      "Number of selected features for regression: 219\n",
      "Number of selected features for regression: 220\n",
      "Number of selected features for regression: 221\n",
      "Number of selected features for regression: 222\n",
      "Number of selected features for regression: 223\n",
      "Number of selected features for regression: 224\n",
      "Number of selected features for regression: 225\n",
      "Number of selected features for regression: 226\n",
      "Number of selected features for regression: 227\n",
      "Number of selected features for regression: 228\n",
      "Number of selected features for regression: 229\n",
      "Number of selected features for regression: 230\n",
      "Number of selected features for regression: 231\n",
      "Number of selected features for regression: 232\n",
      "Number of selected features for regression: 233\n",
      "Number of selected features for regression: 234\n",
      "Number of selected features for regression: 235\n",
      "Number of selected features for regression: 236\n",
      "Number of selected features for regression: 237\n",
      "Number of selected features for regression: 238\n",
      "Number of selected features for regression: 239\n",
      "Number of selected features for regression: 240\n",
      "Number of selected features for regression: 241\n",
      "Number of selected features for regression: 242\n",
      "Number of selected features for regression: 243\n",
      "Number of selected features for regression: 244\n",
      "Number of selected features for regression: 245\n",
      "Number of selected features for regression: 246\n",
      "Number of selected features for regression: 247\n",
      "Number of selected features for regression: 248\n",
      "Number of selected features for regression: 249\n",
      "Number of selected features for regression: 250\n",
      "Number of selected features for regression: 251\n",
      "Number of selected features for regression: 252\n",
      "Number of selected features for regression: 253\n",
      "Number of selected features for regression: 254\n",
      "Number of selected features for regression: 255\n",
      "Number of selected features for regression: 256\n",
      "Number of selected features for regression: 257\n",
      "Number of selected features for regression: 258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rayka\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py:315: UserWarning: Persisting input arguments took 0.60s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of selected features for regression: 259\n",
      "Number of selected features for regression: 260\n",
      "Number of selected features for regression: 261\n",
      "Number of selected features for regression: 262\n",
      "Number of selected features for regression: 263\n",
      "Number of selected features for regression: 264\n",
      "Number of selected features for regression: 265\n",
      "Number of selected features for regression: 266\n",
      "Number of selected features for regression: 267\n",
      "Number of selected features for regression: 268\n",
      "Number of selected features for regression: 269\n",
      "Number of selected features for regression: 270\n",
      "Number of selected features for regression: 271\n",
      "Number of selected features for regression: 272\n",
      "Number of selected features for regression: 273\n",
      "Number of selected features for regression: 274\n",
      "Number of selected features for regression: 275\n",
      "Number of selected features for regression: 276\n",
      "Number of selected features for regression: 277\n",
      "Number of selected features for regression: 278\n",
      "Number of selected features for regression: 279\n",
      "Number of selected features for regression: 280\n",
      "Number of selected features for regression: 281\n",
      "Number of selected features for regression: 282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rayka\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py:315: UserWarning: Persisting input arguments took 0.69s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of selected features for regression: 283\n",
      "Number of selected features for regression: 284\n",
      "Number of selected features for regression: 285\n",
      "Number of selected features for regression: 286\n",
      "Number of selected features for regression: 287\n",
      "Number of selected features for regression: 288\n",
      "Number of selected features for regression: 289\n",
      "Number of selected features for regression: 290\n",
      "Number of selected features for regression: 291\n",
      "Number of selected features for regression: 292\n",
      "Number of selected features for regression: 293\n",
      "Number of selected features for regression: 294\n",
      "Number of selected features for regression: 295\n",
      "Number of selected features for regression: 296\n",
      "Number of selected features for regression: 297\n",
      "Number of selected features for regression: 298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rayka\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py:315: UserWarning: Persisting input arguments took 0.52s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of selected features for regression: 299\n",
      "Number of selected features for regression: 300\n",
      "Number of selected features for regression: 301\n",
      "Number of selected features for regression: 302\n",
      "Number of selected features for regression: 303\n",
      "Number of selected features for regression: 304\n",
      "Number of selected features for regression: 305\n",
      "Number of selected features for regression: 306\n",
      "Number of selected features for regression: 307\n",
      "Number of selected features for regression: 308\n",
      "Number of selected features for regression: 309\n",
      "Number of selected features for regression: 310\n",
      "Number of selected features for regression: 311\n",
      "Number of selected features for regression: 312\n",
      "Number of selected features for regression: 313\n",
      "Number of selected features for regression: 314\n",
      "Number of selected features for regression: 315\n",
      "Number of selected features for regression: 316\n",
      "Number of selected features for regression: 317\n",
      "Number of selected features for regression: 318\n",
      "Number of selected features for regression: 319\n",
      "Number of selected features for regression: 320\n",
      "Number of selected features for regression: 321\n",
      "Number of selected features for regression: 322\n",
      "Number of selected features for regression: 323\n",
      "Number of selected features for regression: 324\n",
      "Number of selected features for regression: 325\n",
      "Number of selected features for regression: 326\n",
      "Number of selected features for regression: 327\n",
      "Number of selected features for regression: 328\n",
      "Number of selected features for regression: 329\n",
      "Number of selected features for regression: 330\n",
      "Number of selected features for regression: 331\n",
      "Number of selected features for regression: 332\n",
      "Number of selected features for regression: 333\n",
      "Number of selected features for regression: 334\n",
      "Number of selected features for regression: 335\n",
      "Number of selected features for regression: 336\n",
      "Number of selected features for regression: 337\n",
      "Number of selected features for regression: 338\n",
      "Number of selected features for regression: 339\n",
      "Number of selected features for regression: 340\n",
      "Number of selected features for regression: 341\n",
      "Number of selected features for regression: 342\n",
      "Number of selected features for regression: 343\n",
      "Number of selected features for regression: 344\n",
      "Number of selected features for regression: 345\n",
      "Number of selected features for regression: 346\n",
      "Number of selected features for regression: 347\n",
      "Number of selected features for regression: 348\n",
      "Number of selected features for regression: 349\n",
      "Number of selected features for regression: 350\n",
      "Number of selected features for regression: 351\n",
      "Number of selected features for regression: 352\n",
      "Number of selected features for regression: 353\n",
      "Number of selected features for regression: 354\n",
      "Number of selected features for regression: 355\n",
      "Number of selected features for regression: 356\n",
      "Number of selected features for regression: 357\n",
      "Number of selected features for regression: 358\n",
      "Number of selected features for regression: 359\n",
      "Number of selected features for regression: 360\n",
      "Number of selected features for regression: 361\n",
      "Number of selected features for regression: 362\n",
      "Number of selected features for regression: 363\n",
      "Number of selected features for regression: 364\n",
      "Number of selected features for regression: 365\n",
      "Number of selected features for regression: 366\n",
      "Number of selected features for regression: 367\n",
      "Number of selected features for regression: 368\n",
      "Number of selected features for regression: 369\n",
      "Number of selected features for regression: 370\n",
      "Number of selected features for regression: 371\n",
      "Number of selected features for regression: 372\n",
      "Number of selected features for regression: 373\n",
      "Number of selected features for regression: 374\n",
      "Number of selected features for regression: 375\n",
      "Number of selected features for regression: 376\n",
      "Number of selected features for regression: 377\n",
      "Number of selected features for regression: 378\n",
      "Number of selected features for regression: 379\n",
      "Number of selected features for regression: 380\n",
      "Number of selected features for regression: 381\n",
      "Number of selected features for regression: 382\n",
      "Number of selected features for regression: 383\n",
      "Number of selected features for regression: 384\n",
      "Number of selected features for regression: 385\n",
      "Number of selected features for regression: 386\n",
      "Number of selected features for regression: 387\n",
      "Number of selected features for regression: 388\n",
      "Number of selected features for regression: 389\n",
      "Number of selected features for regression: 390\n",
      "Number of selected features for regression: 391\n",
      "Number of selected features for regression: 392\n",
      "Number of selected features for regression: 393\n",
      "Number of selected features for regression: 394\n",
      "Number of selected features for regression: 395\n",
      "Number of selected features for regression: 396\n",
      "Number of selected features for regression: 397\n",
      "Number of selected features for regression: 398\n",
      "Number of selected features for regression: 399\n",
      "Number of selected features for regression: 400\n",
      "Number of selected features for regression: 401\n",
      "Number of selected features for regression: 402\n",
      "Number of selected features for regression: 403\n",
      "Number of selected features for regression: 404\n",
      "Number of selected features for regression: 405\n",
      "Number of selected features for regression: 406\n",
      "Number of selected features for regression: 407\n",
      "Number of selected features for regression: 408\n",
      "Number of selected features for regression: 409\n",
      "Number of selected features for regression: 410\n",
      "Number of selected features for regression: 411\n",
      "Number of selected features for regression: 412\n",
      "Number of selected features for regression: 413\n",
      "Number of selected features for regression: 414\n",
      "Number of selected features for regression: 415\n",
      "Number of selected features for regression: 416\n",
      "Number of selected features for regression: 417\n",
      "Number of selected features for regression: 418\n",
      "Number of selected features for regression: 419\n",
      "Number of selected features for regression: 420\n",
      "Number of selected features for regression: 421\n",
      "Number of selected features for regression: 422\n",
      "Number of selected features for regression: 423\n",
      "Number of selected features for regression: 424\n",
      "Number of selected features for regression: 425\n",
      "Number of selected features for regression: 426\n",
      "Number of selected features for regression: 427\n",
      "Number of selected features for regression: 428\n",
      "Number of selected features for regression: 429\n",
      "Number of selected features for regression: 430\n",
      "Number of selected features for regression: 431\n",
      "Number of selected features for regression: 432\n",
      "Number of selected features for regression: 433\n",
      "Number of selected features for regression: 434\n",
      "Number of selected features for regression: 435\n",
      "Number of selected features for regression: 436\n",
      "Number of selected features for regression: 437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rayka\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py:315: UserWarning: Persisting input arguments took 0.77s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of selected features for regression: 438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rayka\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py:315: UserWarning: Persisting input arguments took 1.59s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of selected features for regression: 439\n",
      "Number of selected features for regression: 440\n",
      "Number of selected features for regression: 441\n",
      "Number of selected features for regression: 442\n",
      "Number of selected features for regression: 443\n"
     ]
    }
   ],
   "source": [
    "# Training validation split \n",
    "kf = KFold(n_splits = Number_folds, random_state = random_state, shuffle=True)\n",
    "\n",
    "# Initial value of parameters and lists\n",
    "best_performance = 0\n",
    "accuracy_mean = []\n",
    "num_features_1 = dataset_train_A.shape[1]\n",
    "\n",
    "for Num_important_attr in range(1, num_features_1):\n",
    "    \n",
    "    # initializing mean score\n",
    "    score_mean = 0\n",
    "    \n",
    "    # instantiating pipeline \n",
    "    scaler = preprocessing.MinMaxScaler()\n",
    "    sel1 = VarianceThreshold(threshold = (var_threshold_feature_select * (1 - var_threshold_feature_select)))\n",
    "    fs2 = SelectKBest(score_func = f_regression, k = Num_important_attr)\n",
    "    pipeline = Pipeline([\n",
    "      ('Unsupervised_Feature_Selection', sel1),\n",
    "      ('Feature_Normalization', preprocessing.MinMaxScaler()),\n",
    "      ('Supervised_Feature_Selection', fs2),\n",
    "      ('Regression_Model', LinearRegression())\n",
    "       ])\n",
    "    \n",
    "    print(\"Number of selected features for regression:\", Num_important_attr)\n",
    "    \n",
    "    for train_index, val_index in kf.split(dataset_train2):\n",
    "        \n",
    "        # loading Training and Validation sets\n",
    "        X_train, X_val = dataset_train2[train_index], dataset_train2[val_index]\n",
    "        y_train_B, y_val = y_train[train_index], y_train[val_index]\n",
    "\n",
    "        # fitting model to each fold\n",
    "        pipeline.fit(X_train, y_train_B)\n",
    "        score_mean += pipeline.score(X_val, y_val)/Number_folds \n",
    "    \n",
    "    accuracy_mean.append(score_mean)\n",
    "    \n",
    "    # saving the best model among all hyperparameters\n",
    "    if score_mean > best_performance:\n",
    "        best_performance = score_mean\n",
    "        dump (pipeline, experiment_path + 'Best_Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is worth mentioning that this is one of the simplest models for predicting the KWH field. Regularization terms can be added to the loss function to develop a simpler model as well.\n",
    "\n",
    "Once the model is chosen based on the cross-validation module's score, the selected model with tunned parameters can be applied to the whole training set. Therefore, the final predictive model for KWH is ready. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('Unsupervised_Feature_Selection',\n",
       "                 VarianceThreshold(threshold=0.15999999999999998)),\n",
       "                ('Feature_Normalization',\n",
       "                 MinMaxScaler(copy=True, feature_range=(0, 1))),\n",
       "                ('Supervised_Feature_Selection',\n",
       "                 SelectKBest(k=300,\n",
       "                             score_func=<function f_regression at 0x000001FA8CD279D8>)),\n",
       "                ('Regression_Model',\n",
       "                 LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
       "                                  normalize=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#finding optimum k for the architecture\n",
    "optimum_Num_important_attr = 300\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "sel1 = VarianceThreshold(threshold = (var_threshold_feature_select * (1 - var_threshold_feature_select)))\n",
    "fs2 = SelectKBest(score_func = f_regression, k = optimum_Num_important_attr)\n",
    "pipeline_main = Pipeline([\n",
    "  ('Unsupervised_Feature_Selection', sel1),\n",
    "  ('Feature_Normalization', preprocessing.MinMaxScaler()),\n",
    "  ('Supervised_Feature_Selection', fs2),\n",
    "  ('Regression_Model', LinearRegression())\n",
    "   ])\n",
    "\n",
    "#save model\n",
    "dump (pipeline_main, experiment_path + 'Best_Model_main')\n",
    "\n",
    "#fitting model to the complete training set\n",
    "pipeline_main.fit(dataset_train2, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result\n",
    "\n",
    "The cross-validation score is plot across the number of selected features. As we can observe below,  two main breaks in the diagram refer to candidates for the optimum number of selected features. Additionally, the mean score of cross-validation approaches 1, which justifies that the model is fitted properly. Hence, there is no need to apply other complicated models. \n",
    "\n",
    "The final pipeline applied to the test set results in the best $R^2$ score of  0.9964. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score of the model: 0.9999999998676401\n",
      "Final score for test set 0.9999999998383979\n",
      "Final score for test set for main pipeline 0.9964487798053792\n"
     ]
    }
   ],
   "source": [
    "# Plot number of features VS. cross-validation scores\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number of selected features\")\n",
    "plt.ylabel(\"Cross validation score (mean coefficient R^2)\")\n",
    "plt.plot(range(1,\n",
    "               num_features_1),\n",
    "         accuracy_mean)\n",
    "plt.show()\n",
    "\n",
    "print( \"Best score of the model:\", best_performance)\n",
    "\n",
    "# load model\n",
    "pipeline1 = load(experiment_path + 'Best_Model') \n",
    "\n",
    "# prediction over test set\n",
    "print(\"Final score for test set\", pipeline1.score(dataset_test2,y_test))\n",
    "\n",
    "print(\"Final score for test set for main pipeline\", pipeline_main.score(dataset_test2,y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
